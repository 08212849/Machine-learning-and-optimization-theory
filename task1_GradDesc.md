# å®éªŒä¸€

## 1ã€æ•°æ®é›†åŠé¢„å¤„ç†

æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›†æ˜¯è‘—åçš„ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ åŸºå‡†æ•°æ®é›†ï¼Œå®ƒåŒ…å«äº†506ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æœ‰13ä¸ªç‰¹å¾å˜é‡ï¼Œè¿™äº›å˜é‡å¯èƒ½å½±å“æˆ¿å±‹ä»·æ ¼ã€‚è¿™äº›ç‰¹å¾åŒ…æ‹¬çŠ¯ç½ªç‡ã€åœ°æ–¹è´¢äº§ç¨ç‡ã€å­¦ç”Ÿ-æ•™å¸ˆæ¯”ä¾‹ç­‰ï¼Œè€Œç›®æ ‡å˜é‡æ˜¯æˆ¿å±‹ä»·æ ¼çš„ä¸­ä½æ•°ã€‚åœ¨æœ¬æ¬¡å®éªŒä¸­ï¼Œé‡‡ç”¨çº¿æ€§å›å½’æ¨¡å‹æ¥é¢„æµ‹åŸºäºè¿™äº›å› ç´ çš„æˆ¿å±‹ä»·æ ¼ã€‚

<img src="C:/Users/86199/AppData/Roaming/Typora/typora-user-images/image-20240505153816734.png" alt="image-20240505153816734" style="zoom: 67%;" />

å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸¤éƒ¨åˆ†ï¼Œæ¯”ä¾‹ä¸º8:2ã€‚

ä¸ºäº†æ¶ˆé™¤ä¸åŒé‡çº²å’Œé‡çº§çš„å½±å“ï¼Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œç®—æ³•çš„æ”¶æ•›é€Ÿåº¦ï¼Œå¯¹æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›†ä¸­çš„ç‰¹å¾è¿›è¡Œäº†æ ‡å‡†å½’ä¸€åŒ–å¤„ç†ã€‚è¿™ä¸€è¿‡ç¨‹é€šè¿‡å‡å»ç‰¹å¾çš„å‡å€¼å¹¶é™¤ä»¥å…¶æ ‡å‡†å·®ï¼Œå°†æ•°æ®è½¬æ¢è‡³ä¸€ä¸ªå‡å€¼ä¸º0ã€æ–¹å·®ä¸º1çš„æ­£æ€åˆ†å¸ƒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç¡®ä¿å„ä¸ªç‰¹å¾åœ¨æ¨¡å‹è®­ç»ƒä¸­å…·æœ‰ç›¸åŒçš„é‡è¦æ€§æƒé‡ï¼Œé¿å…äº†æŸäº›ç‰¹å¾ç”±äºæ•°å€¼èŒƒå›´å¤§è€Œå¯¹æ¨¡å‹äº§ç”Ÿä¸æˆæ¯”ä¾‹çš„å½±å“ã€‚æ­¤å¤–ï¼Œå¯¹äºæ¢¯åº¦ä¸‹é™ï¼Œç¼©æ”¾è¿˜å¯ä»¥åŠ é€Ÿæ”¶æ•›è¿‡ç¨‹ã€‚

```python
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import fetch_openml

# ä» OpenML è·å–æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›†
boston = fetch_openml(name='boston', version=1) 
X, y = boston.data, boston.target
# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

## 2ã€æ¨¡å‹æ„å»º

- ä»¥ç±»çš„æ–¹å¼å®ç°ç½‘ç»œï¼Œä½¿ç”¨æ—¶å¯ä»¥ç”Ÿæˆå¤šä¸ªæ¨¡å‹ç¤ºä¾‹ã€‚

- ç®€å•çš„ç±»æˆå‘˜å˜é‡æœ‰wå’Œbï¼Œåœ¨ç±»åˆå§‹åŒ–å‡½æ•°æ—¶åˆå§‹åŒ–å˜é‡ã€‚

- `__init__` æ–¹æ³•åˆå§‹åŒ–äº†æ¨¡å‹çš„æƒé‡å’Œåç½®ï¼ŒåŒæ—¶è®¾ç½®äº†å­¦ä¹ ç‡ã€‚

- `forward` æ–¹æ³•å®ç°äº†æ¨¡å‹çš„å‰å‘è®¡ç®—ï¼Œå®ƒæ¥å—è¾“å…¥æ•°æ® `X` å¹¶è¿”å›é¢„æµ‹å€¼ã€‚

- `loss` æ–¹æ³•è®¡ç®—äº†å½“å‰æƒé‡å’Œåç½®ä¸‹çš„æŸå¤±å‡½æ•°å€¼ã€‚

- `backward` æ–¹æ³•å®ç°äº†åå‘ä¼ æ’­ç®—æ³•ï¼Œè®¡ç®—äº†æŸå¤±å‡½æ•°å…³äºæƒé‡å’Œåç½®çš„æ¢¯åº¦ã€‚

- `update` æ–¹æ³•æ ¹æ®è®¡ç®—å‡ºçš„æ¢¯åº¦æ›´æ–°æƒé‡å’Œåç½®ï¼Œå¯æ ¹æ®ä¸åŒä¼˜åŒ–æ–¹å¼è®¾ç½®å‡½æ•°ã€‚

- `train` æ–¹æ³•è´Ÿè´£æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬å‰å‘è®¡ç®—ã€åå‘ä¼ æ’­ã€å‚æ•°æ›´æ–°ä»¥åŠæŸå¤±å‡½æ•°çš„è®¡ç®—å’Œæ‰“å°ã€‚

  æ•´ä¸ªç½‘ç»œå®ç°ç»“æ„å¦‚å›¾ï¼š

```python
class Network(object):
    def __init__(self, num_of_weights, optimizer, iterations=5000, learning_rate=0.25):...

    def forward(self, x):...

    def loss(self, z, y):...

    def gradient(self, x, y):...

    def update_momentum(self, gradient_w, gradient_b, eta=0.01, beta=0.9):...

    def update_gd(self, gradient_w, gradient_b, eta=0.01):...

    def update_adagrad(self, gradient_w, gradient_b, eps=1e-8):...
    
    def update_adam(self, gradient_w, gradient_b, t, beta1=0.09, beta2=0.05, eps=1e-8):...

    def train(self, x, y, eta=0.01):...

    def drawGDResult(self):...

    def drawLoss(self, losses):...
```

## 3ã€æŸå¤±å‡½æ•°

åœ¨å›å½’é—®é¢˜ä¸­ï¼Œå‡æ–¹è¯¯å·®ï¼ˆMean Squared Error, MSEï¼‰æ˜¯ä¸€ç§å¸¸ç”¨çš„æŸå¤±å‡½æ•°ï¼Œç”¨äºè¡¡é‡æ¨¡å‹é¢„æµ‹å€¼ä¸å®é™…å€¼ä¹‹é—´çš„å·®å¼‚ã€‚å®ƒæ˜¯é¢„æµ‹è¯¯å·®çš„åº¦é‡ï¼Œå¯ä»¥ä¸ºå›å½’æ¨¡å‹çš„è®­ç»ƒæä¾›æŒ‡å¯¼ã€‚

å‡æ–¹è¯¯å·®æ˜¯æ‰€æœ‰é¢„æµ‹è¯¯å·®ï¼ˆæ®‹å·®ï¼‰å¹³æ–¹çš„å¹³å‡å€¼ï¼Œå…¶å…¬å¼å®šä¹‰å¦‚ä¸‹ï¼š

![image-20240505204135830](C:/Users/86199/AppData/Roaming/Typora/typora-user-images/image-20240505204135830.png)

å…¶ä¸­ï¼š

- ğ‘›*n* æ˜¯æ ·æœ¬çš„æ€»æ•°ã€‚
- ğ‘¦ğ‘–*y**i* æ˜¯ç¬¬ ğ‘–*i* ä¸ªæ ·æœ¬çš„å®é™…è§‚æµ‹å€¼ã€‚
- ğ‘¦^ğ‘–*y*^*i* æ˜¯ç¬¬ ğ‘–*i* ä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼ã€‚

å‡æ–¹è¯¯å·®å¯¹è¾ƒå¤§çš„è¯¯å·®ç»™äºˆæ›´å¤§çš„æƒ©ç½šï¼Œå› ä¸ºå®ƒå¯¹è¯¯å·®å¹³æ–¹æ±‚å¹³å‡ã€‚è¿™æ„å‘³ç€å®ƒå¯¹å¼‚å¸¸å€¼ï¼ˆoutliersï¼‰æ•æ„Ÿï¼Œå¯èƒ½ä¼šå› ä¸ºä¸ªåˆ«å¤§çš„è¯¯å·®è€Œæ˜¾è‘—å¢åŠ æŸå¤±ã€‚åŒæ—¶ä¹Ÿæ˜¯ä¸€ä¸ªå¯å¾®çš„å‡½æ•°ï¼Œè¿™ä½¿å¾—å®ƒé€‚åˆäºä½¿ç”¨æ¢¯åº¦ä¸‹é™ç­‰ä¼˜åŒ–ç®—æ³•è¿›è¡Œæœ€å°åŒ–ã€‚è®¡ç®—ç›¸å¯¹ç®€å•ï¼Œä¸”æ˜“äºç†è§£å’Œå®ç°ã€‚ä»£ç å®ç°å¦‚ä¸‹ï¼š

```python
    def loss(self, z, y):
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
```

## 4ã€ä¼˜åŒ–ç®—æ³•é€‰æ‹©

æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descent, GDï¼‰ã€Adamï¼ˆè‡ªé€‚åº”çŸ©ä¼°è®¡ï¼‰ã€Momentumï¼ˆåŠ¨é‡ï¼‰å’Œ AdaGradï¼ˆè‡ªé€‚åº”æ¢¯åº¦ç®—æ³•ï¼‰ æ˜¯å¸¸ç”¨çš„æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºåœ¨æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´å‚æ•°ï¼Œä¸‹é¢å¯¹è¿™äº›æ–¹æ³•è¿›è¡Œå®éªŒæ¯”è¾ƒã€‚

### 4.1 GD

æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§ä¸€é˜¶ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºæœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚å®ƒé€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ï¼Œå¹¶æ²¿ç€æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°å‚æ•°æ¥å·¥ä½œã€‚å®ç°ç®€å•ï¼Œå¯¹äºå­¦ä¹ ç‡çš„é€‰æ‹©å¾ˆå…³é”®ï¼Œå¦‚æœè®¾ç½®ä¸å½“ï¼Œå¯èƒ½å¯¼è‡´è¶…è°ƒæˆ–æ”¶æ•›åˆ°æ¬¡ä¼˜è§£ï¼›å¯¹äºéå‡¸å‡½æ•°ï¼Œå¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€å°å€¼ã€‚

æ›´æ–°å‡½æ•°å¦‚ä¸‹ï¼š

```python
    def update_gd(self, gradient_w, gradient_b, eta=0.01):
        self.w = self.w - eta * gradient_w
        self.b = self.b - eta * gradient_b
```

### 4.2 Momentum

Momentum æ˜¯ä¸€ç§ä¼˜åŒ–æ¢¯åº¦ä¸‹é™çš„ç®—æ³•ï¼Œé€šè¿‡å°†å½“å‰æ¢¯åº¦ä¸ä¹‹å‰æ¢¯åº¦çš„æŒ‡æ•°åŠ æƒå¹³å‡ç»“åˆèµ·æ¥ï¼Œå‡å°‘æ¢¯åº¦æ›´æ–°çš„æŠ–åŠ¨ã€‚å¯¹äºå…·æœ‰é«˜æ›²ç‡çš„åŒºåŸŸï¼Œå¯ä»¥æ›´å¿«åœ°ç§»åŠ¨ï¼›å¯¹äºå¹³å¦çš„åŒºåŸŸï¼Œåˆ™å‡é€Ÿã€‚æ­¤å¤–éœ€è¦è°ƒæ•´é¢å¤–çš„å‚æ•°ï¼ˆåŠ¨é‡ç³»æ•°ï¼‰ã€‚

æ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š

![image-20240505211208197](C:/Users/86199/AppData/Roaming/Typora/typora-user-images/image-20240505211208197.png)

å…¶ä¸­ï¼Œğ‘šğ‘¡ æ˜¯åŠ¨é‡é¡¹ï¼Œğ›½æ˜¯åŠ¨é‡ç³»æ•°ï¼Œå®ç°å¦‚ä¸‹ï¼š

```python
    def update_momentum(self, gradient_w, gradient_b, eta=0.01, beta=0.9):
        momentum_w = beta * self.momentum_w - eta * gradient_w
        momentum_b = beta * self.momentum_b - eta * gradient_b
        self.momentum_w = momentum_w
        self.momentum_b = momentum_b
        self.w = self.w + momentum_w
        self.b = self.b + momentum_b
```

### 4.3 Adam

Adam æ˜¯ä¸€ç§ç»“åˆäº†åŠ¨é‡å’Œ RMSProp æ€æƒ³çš„ä¼˜åŒ–ç®—æ³•ã€‚Adam ç»“åˆäº†åŠ¨é‡æ–¹æ³•ä¸­çš„æ¦‚å¿µï¼Œå³è€ƒè™‘è¿‡å»æ¢¯åº¦çš„æŒ‡æ•°è¡°å‡å¹³å‡ï¼Œä½¿å¾—æ›´æ–°æ–¹å‘æ›´åŠ å¹³æ»‘ï¼ŒåŒæ—¶ç»“åˆ RMSProp ä¸­çš„æ¦‚å¿µï¼Œå³è€ƒè™‘è¿‡å»æ¢¯åº¦çš„å¹³æ–¹çš„æŒ‡æ•°è¡°å‡å¹³å‡ï¼Œä½¿å¾—å­¦ä¹ ç‡è‡ªé€‚åº”è°ƒæ•´ã€‚å®ƒè®¡ç®—äº†æ¢¯åº¦çš„ä¸€é˜¶çŸ©ï¼ˆåŠ¨é‡ï¼‰å’ŒäºŒé˜¶çŸ©ï¼ˆæ–¹å·®ï¼‰ï¼Œå¹¶ä½¿ç”¨è¿™äº›çŸ©æ¥è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ã€‚

æ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š

![image-20240505211730682](C:/Users/86199/AppData/Roaming/Typora/typora-user-images/image-20240505211730682.png)

å…¶ä¸­ï¼Œğ‘šğ‘¡ å’Œ ğ‘£ğ‘¡ åˆ†åˆ«æ˜¯æ¢¯åº¦çš„ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡ï¼Œğ›½1 å’Œ ğ›½2 æ˜¯è¡°å‡ç‡ï¼Œå®ç°å¦‚ä¸‹ï¼š

```python
    def update_adam(self, gradient_w, gradient_b, t, beta1=0.9, beta2=0.999, eps=1e-8):
        self.m_w = beta1 * self.m_w + (1 - beta1) * gradient_w
        self.m_b = beta1 * self.m_b + (1 - beta1) * gradient_b
        self.v_w = beta2 * self.v_w + (1 - beta2) * (gradient_w ** 2)
        self.v_b = beta2 * self.v_b + (1 - beta2) * (gradient_b ** 2)
        t += 1
        alpha_w = self.learning_rate * (np.sqrt(1 - beta1 ** t) ** 0.5 * (self.m_w / (np.sqrt(self.v_w) + eps)))
        alpha_b = self.learning_rate * (np.sqrt(1 - beta1 ** t) ** 0.5 * (self.m_b / (np.sqrt(self.v_b) + eps)))
        self.w -= alpha_w
        self.b -= alpha_b
        return t
```

### 4.4 Adagrad

AdaGrad é€šè¿‡ä¸ºæ¯ä¸ªå‚æ•°ç‹¬ç«‹åœ°è°ƒæ•´å­¦ä¹ ç‡æ¥ä¼˜åŒ–æ¢¯åº¦ä¸‹é™ã€‚å®ƒç´¯ç§¯å†å²æ¢¯åº¦çš„å¹³æ–¹ï¼Œå¹¶ç”¨è¿™äº›ä¿¡æ¯æ¥è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ã€‚ç¼ºç‚¹æ˜¯ç´¯ç§¯æ¢¯åº¦å¯èƒ½ä¼šå¯¼è‡´å­¦ä¹ ç‡è¿‡æ—©åœ°å˜å¾—éå¸¸å°ï¼Œä»è€Œå‡æ…¢å­¦ä¹ è¿‡ç¨‹æˆ–å¯¼è‡´éš¾ä»¥æ”¶æ•›ã€‚

æ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š

![image-20240505212122800](C:/Users/86199/AppData/Roaming/Typora/typora-user-images/image-20240505212122800.png)

å…¶ä¸­ï¼Œğºğ‘¡æ˜¯ç´¯ç§¯çš„æ¢¯åº¦å¹³æ–¹å’Œï¼Œğœ–æ˜¯å°å¸¸æ•°ï¼Œé¿å…åˆ†æ¯ä¸ºé›¶ã€‚å®ç°å¦‚ä¸‹ï¼š

```python
    def update_adagrad(self, gradient_w, gradient_b, eps=1e-8):
        self.accum_grad_w += gradient_w ** 2
        self.accum_grad_b += gradient_b ** 2
        self.w -= self.learning_rate / (np.sqrt(self.accum_grad_w) + eps) * gradient_w
        self.b -= self.learning_rate / (np.sqrt(self.accum_grad_b) + eps) * gradient_b
```

### 4.5 ç®—æ³•å¯¹æ¯”

åœ¨è¿­ä»£æ¬¡æ•°åˆ†ä¸º50ã€500ã€5000çš„æƒ…å†µä¸‹ï¼Œä¸åŒä¼˜åŒ–ç®—æ³•é¢„æµ‹ç»“æœå¦‚å›¾ã€‚

![Figure_50](D:/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E4%BC%98%E5%8C%96/%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C/Figure_50.png)

![Figure_500](D:/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E4%BC%98%E5%8C%96/%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C/Figure_500.png)

![Figure_2000](D:/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E4%BC%98%E5%8C%96/%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C/Figure_2000.png)

momentum loss: 21.656081762580687
sgd loss: 21.9054372169007
adam loss: 22.83821015541945
adagrad loss: 21.64172587614321